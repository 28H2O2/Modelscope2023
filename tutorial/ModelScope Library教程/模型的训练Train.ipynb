{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# è®­ç»ƒä»‹ç»\n",
        "\n",
        "ModelScopeæä¾›äº†å¾ˆå¤šæ¨¡å‹ï¼Œå¤§éƒ¨åˆ†æ¨¡å‹å¯ä»¥ç›´æ¥åœ¨[æ¨ç†ä¸­ä½¿ç”¨](./æ¨¡å‹çš„æ¨ç†Pipeline.ipynb)ï¼Œä½†æ˜¯æœ‰ä¸€äº›æ¨¡å‹è¢«åŠ è½½åˆ°ä»£ç ä¸­åï¼Œæœ‰ä¸€å°éƒ¨åˆ†çš„æ¨¡å‹å‚æ•°éœ€è¦æ ¹æ®æ•°æ®é›†è¿›è¡Œé‡æ–°å®šåˆ¶ï¼Œè¿™ä¸ªè¿‡ç¨‹å«åšfinetuneï¼ˆå¾®è°ƒï¼‰ã€‚\n",
        "\n",
        "ä¸¾ä¾‹æ¥è¯´ï¼Œç”¨æˆ·åœ¨modelhubä¸­æœç´¢ï¼Œä¼šå‘ç°æœ‰ä¸€äº›æ¨¡å‹åç§°ä¸­åŒ…å«\"backbone\"å­—æ ·ã€‚å¦‚æœæŠŠè¿™ä¸ªæ¨¡å‹åŠ è½½åˆ°ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¿™ä¸ªåˆ†ç±»ä»»åŠ¡æ¨¡å‹æœ€åçš„ä¸€éƒ¨åˆ†å‚æ•°ï¼ˆä¸€èˆ¬æˆ‘ä»¬æŠŠè¿™éƒ¨åˆ†å«åˆ†ç±»å™¨ï¼‰æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œè¿™ç§éšæœºå‚æ•°ä¸èƒ½ç”¨æ¥ç›´æ¥è¿›è¡Œé¢„æµ‹ï¼Œè€Œè¦å…ˆ\n",
        "ç»è¿‡ç”¨æˆ·æä¾›çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆçš„æ¨¡å‹æ‰èƒ½ç”¨æ¥é¢„æµ‹å’Œä½¿ç”¨ã€‚\n",
        "\n",
        "é‚£ä¹ˆä¸ºä»€ä¹ˆä¸æä¾›å®Œæ•´çš„æ¨¡å‹ç›´æ¥ä½¿ç”¨å‘¢ï¼Ÿè¿™æ¶‰åŠåˆ°äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé—®é¢˜ï¼Œä¹Ÿå°±æ˜¯\"æ³›åŒ–æ€§\"ã€‚æ¯”å¦‚ModelScopeå®˜æ–¹æä¾›äº†ä¸€ä¸ªå¯¹é€šç”¨åŒ»ç–—å›¾ç‰‡è¿›è¡Œåˆ†ç±»çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹åœ¨é€šå¸¸çš„åŒ»ç–—åœºæ™¯ä¸‹å¯ä»¥å¾ˆå¥½çš„è¿è¡Œï¼Œä½†å‡å¦‚ç”¨æˆ·çš„åœºæ™¯æ˜¯åŒ»ç–—ä¸‹é¢çš„ç»†åˆ†é¢†åŸŸï¼Œ\n",
        "æ¯”å¦‚ç‰™åŒ»ï¼Œé‚£ä¹ˆç‰™åŒ»ç‰¹æœ‰çš„å›¾ç‰‡åœ¨è¿™ä¸ªé€šç”¨æ¨¡å‹ä¸‹åˆ†ç±»æ•ˆæœå¯èƒ½ä¼šä¸å¥½ï¼Œè¿™å°±æ˜¯æ³›åŒ–æ€§é—®é¢˜ã€‚æ³›åŒ–æ€§é—®é¢˜åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ˜¯å¿…ç„¶å­˜åœ¨çš„ï¼Œä»»ä½•æ¨¡å‹éƒ½ä¼šå­˜åœ¨è¿™ä¸ªé—®é¢˜ã€‚äºæ˜¯å°±ä¼šæœ‰ç±»ä¼¼è¿™æ ·çš„è§£å†³æ–¹æ¡ˆï¼šé¦–å…ˆä½¿ç”¨åŒ»ç–—é¢†åŸŸçš„å¤§é‡æ•°æ®é›†è®­ç»ƒä¸€ä¸ª\n",
        "é€šç”¨çš„æ¨¡å‹ï¼Œä½†è¿™ä¸ªæ¨¡å‹ç¼ºå°‘æœ€ä¸Šé¢çš„åˆ†ç±»å™¨ã€‚ç”¨æˆ·åªéœ€è¦æä¾›ç‰™åŒ»çš„å°‘é‡æ•°æ®ï¼Œå°±å¯ä»¥åœ¨è¿™ä¸ªé€šç”¨æ¨¡å‹åŸºç¡€ä¸Šå°†åˆ†ç±»å™¨è®­ç»ƒï¼ˆå¾®è°ƒï¼‰å‡ºæ¥ï¼Œè¾¾åˆ°ç‰¹å®šåœºæ™¯é€‚é…çš„è¦æ±‚ï¼Œè¿™ä¸ªæ•´ä½“æ–¹æ¡ˆå°±æ˜¯é¢„è®­ç»ƒï¼Œæä¾›çš„é€šç”¨æ¨¡å‹å°±æ˜¯backboneã€‚å½“ç„¶ï¼Œé¢„è®­ç»ƒçš„é€šç”¨æ¨¡å‹è®­ç»ƒæ˜¯æ— æ ‡æ³¨çš„ï¼Œå¾®è°ƒè¿‡ç¨‹çš„è®­ç»ƒæ˜¯\n",
        "æœ‰æ ‡æ³¨çš„ï¼Œå…³äºè¿™äº›çŸ¥è¯†ç”¨æˆ·å¯ä»¥è‡ªè¡ŒæŸ¥é˜…ã€‚\n",
        "\n",
        "åœ¨éé¢„è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆä¸­ï¼Œæ¯”å¦‚LSTMã€RNNç­‰è§„æ¨¡è¾ƒå°çš„æ¨¡å‹ï¼Œå®ƒçš„å‚æ•°æ˜¯æ¥è¿‘éšæœºåˆå§‹åŒ–çš„ï¼Œè¿™æ ·çš„æ¨¡å‹è¦ç»è¿‡ç‰¹å®šæ•°æ®é›†çš„è®­ç»ƒæ‰èƒ½å¤Ÿä½¿ç”¨ã€‚\n",
        "\n",
        "ä¸€èˆ¬æ¥è¯´ï¼Œè®­ç»ƒè¿‡ç¨‹åŒ…å«äº†è®­ç»ƒ(train)å’Œè¯„ä¼°(evaluate)ä¸¤ä¸ªè¿‡ç¨‹ã€‚\n",
        "ç”¨æˆ·å¯ä»¥ç†è§£ä¸ºè®­ç»ƒè¿‡ç¨‹æ˜¯æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œè¯„ä¼°è¿‡ç¨‹ç©¿æ’åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ–åœ¨è®­ç»ƒè¿‡ç¨‹ä¹‹åï¼Œç»™å‡ºå½“å‰æ¨¡å‹å¥½åçš„æ—¥å¿—ï¼Œæˆ–é€šè¿‡è¯„ä¼°ç»“æœå­˜å‚¨æ•ˆæœæœ€å¥½çš„æ¨¡å‹ä¾¿äºåç»­è®­ç»ƒæˆ–ä½¿ç”¨ã€‚\n",
        "å’Œè®­ç»ƒè¿‡ç¨‹ç©¿æ’è¿›è¡Œçš„è¯„ä¼°è¿‡ç¨‹å«åšäº¤å‰éªŒè¯ã€‚\n",
        "\n",
        "ModelScopeæä¾›äº†å®Œæ•´çš„è®­ç»ƒç»„ä»¶ï¼Œå…¶ä¸­çš„ä¸»è¦ç»„ä»¶è¢«ç§°ä¸ºtrainerï¼ˆè®­ç»ƒå™¨ï¼‰ï¼Œè¿™äº›ç»„ä»¶å¯ä»¥åœ¨`é¢„è®­ç»ƒ`æˆ–`æ™®é€šè®­ç»ƒ`åœºæ™¯ä¸‹ä½¿ç”¨ã€‚\n",
        "\n",
        "# ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼šæ–‡æœ¬åˆ†ç±»\n",
        "ä¸‹é¢ä»¥ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•é€šè¿‡åå‡ è¡Œä»£ç ï¼Œå°±å¯ä»¥ç«¯åˆ°ç«¯æ‰§è¡Œä¸€ä¸ªfinetuneä»»åŠ¡ã€‚æ•´ä½“æµç¨‹åŒ…å«ä¸€ä¸‹æ­¥éª¤ï¼š\n",
        "\n",
        "- è½½å…¥æ•°æ®é›†\n",
        "- æ•°æ®é¢„å¤„ç†\n",
        "- è®­ç»ƒ\n",
        "- è¯„ä¼°\n",
        "## è½½å…¥æ•°æ®é›†\n",
        "ModelScopeå¯ä»¥æä¾›äº†æ ‡å‡†çš„`MsDataset`æ¥å£ä¾›ç”¨æˆ·è¿›è¡ŒåŸºäºModelScopeç”Ÿæ€çš„æ•°æ®æºåŠ è½½ã€‚ä¸‹é¢ä»¥åŠ è½½NLPé¢†åŸŸçš„afqmcï¼ˆAnt Financial Question Matching Corpusï¼‰æ•°æ®é›†ä¸ºä¾‹è¿›è¡Œæ¼”ç¤º\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.msdatasets import MsDataset\n",
        "# è½½å…¥è®­ç»ƒæ•°æ®ï¼Œæ³¨æ„è¿™ä¸ªè®­ç»ƒæ•°æ®é›†ä¸æ˜¯å®Œæ•´çš„ï¼Œè€Œæ˜¯ä¸€ä¸ªå°è§„æ¨¡æµ‹è¯•ç”¨çš„æ•°æ®é›†\n",
        "train_dataset = MsDataset.load('clue',  subset_name='afqmc', split='train')\n",
        "# è½½å…¥è¯„ä¼°æ•°æ®\n",
        "eval_dataset = MsDataset.load('clue',  subset_name='afqmc', split='validation')\n",
        "\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "å…·ä½“MsDataset ä½¿ç”¨å¯ä»¥å‚è€ƒæ¥å£æ–‡æ¡£ï¼š [æ•°æ®çš„å¤„ç†](../æ•°æ®é›†/æ•°æ®é›†ä»‹ç».ipynb)\n",
        "## æ•°æ®é¢„å¤„ç†\n",
        "åœ¨ModelScopeä¸­ï¼Œæ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹å¼ºç›¸å…³ï¼Œå› æ­¤ï¼Œåœ¨æŒ‡å®šæ¨¡å‹ä»¥åï¼ŒModelScopeæ¡†æ¶ä¼šè‡ªåŠ¨ä»å¯¹åº”çš„æ¨¡å‹çš„æ¨¡å‹å¡ç‰‡(modelcard)ä¸­è¯»å–é…ç½®æ–‡ä»¶ä¸­çš„preprocessorå…³é”®å­—ï¼Œè‡ªåŠ¨å®Œæˆé¢„å¤„ç†çš„å®ä¾‹åŒ–ã€‚\n",
        "\n",
        "é¢„å¤„ç†å…·ä½“ä»‹ç»å¯ä»¥å‚è€ƒ [æ•°æ®é¢„å¤„ç†å™¨](./æ•°æ®çš„é¢„å¤„ç†.ipynb)ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# æŒ‡å®šæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œé¢„å¤„ç†è¿‡ç¨‹trainerä¼šè‡ªåŠ¨å®Œæˆ\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-tiny'\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "é…ç½®æ–‡ä»¶çš„ç›¸å…³å­—æ®µï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "...\n",
        "\"preprocessor\":{\n",
        "    \"type\": \"sen-cls-tokenizer\",\n",
        "  },\n",
        "...\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "å½“ç„¶ï¼Œå¯¹äºè¿›é˜¶çš„ä½¿ç”¨è€…ï¼Œé…ç½®æ–‡ä»¶ä¹Ÿæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰å¹¶ä»ä»»æ„æœ¬åœ°è·¯å¾„è¯»å–ï¼Œå…·ä½“å‚è€ƒæ–‡æ¡£ï¼š[configurationè¯¦è§£](../å¼€å‘è€…ä½¿ç”¨æŒ‡å—/Configurationè¯¦è§£.ipynb)\n",
        "## è®­ç»ƒ\n",
        "é¦–å…ˆï¼Œé…ç½®è®­ç»ƒæ‰€éœ€å‚æ•°ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.trainers import build_trainer\n",
        "\n",
        "# æŒ‡å®šå·¥ä½œç›®å½•\n",
        "tmp_dir = \"/tmp\"\n",
        "\n",
        "# é…ç½®å‚æ•°\n",
        "kwargs = dict(\n",
        "        model=model_id,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        work_dir=tmp_dir)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "æ ¹æ®å‚æ•°å®ä¾‹åŒ–trainerå¯¹è±¡\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "trainer = build_trainer(default_args=kwargs)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "æœ€åï¼Œè°ƒç”¨trainæ¥å£è¿›è¡Œè®­ç»ƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "æ­å–œï¼Œä½ å®Œæˆäº†ä¸€æ¬¡æ¨¡å‹è®­ç»ƒğŸ˜€\n",
        "\n",
        "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šä¾‹ä¸­`nlp_structbert_sentence-similarity_chinese-tiny`æ¨¡å‹æ˜¯ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå› æ­¤ä¸å¿…å†è¿›è¡Œè®­ç»ƒå¯ç›´æ¥ä½¿ç”¨ã€‚\n",
        "è¿™ç±»NLPæ¨¡å‹ä¸­ä¸€èˆ¬å¸¦æœ‰label_mapping.jsonæ–‡ä»¶è¡¨ç¤ºlabelå’Œidæ˜ å°„å…³ç³»ç”¨äºæ¨ç†ï¼Œç”¨æˆ·å¦‚æœè®­ç»ƒå®ƒå¯èƒ½ä¼šæŠ¥é”™labelä¸åŒ¹é…ã€‚\n",
        "è¿™æ—¶éœ€è¦æ›´æ–°æ•°æ®é›†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ\u003ca href=\"#load_dataset\"\u003eè½½å…¥æ•°æ®é›†\u003c/a\u003eçš„ç« èŠ‚ã€‚\n",
        "\n",
        "## è¯„ä¼°\n",
        "è®­ç»ƒå®Œæˆä»¥åï¼Œé…ç½®è¯„ä¼°æ•°æ®é›†ï¼Œç›´æ¥è°ƒç”¨trainerå¯¹è±¡çš„evaluateå‡½æ•°ï¼Œå³å¯å®Œæˆæ¨¡å‹çš„è¯„ä¼°ï¼Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# ç›´æ¥è°ƒç”¨trainer.evaluateï¼Œå¯ä»¥ä¼ å…¥trainé˜¶æ®µç”Ÿæˆçš„ckpt\n",
        "# ä¹Ÿå¯ä»¥ä¸ä¼ å…¥å‚æ•°ï¼Œç›´æ¥éªŒè¯model\n",
        "metrics = trainer.evaluate(checkpoint_path=None)\n",
        "print(metrics)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## å½“å‰ç‰ˆæœ¬ModelScopeæ”¯æŒfinetuneçš„ä»»åŠ¡å’Œæ¨¡å‹åˆ—è¡¨\n",
        "ä»¥ä¸‹æ¨¡å‹è¯¦ç»†è°ƒç”¨ä¿¡æ¯è¯·å‚è€ƒå¹³å°å®˜ç½‘[https://www.modelscope.cn/](https://www.modelscope.cn/#/models)ï¼Œå¯¹åº”æ¨¡å‹çš„æ¨¡å‹å¡ç‰‡ï¼ˆModelCardï¼‰ã€‚æ”¯æŒfinetuneå’Œè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨å…¶æ¨¡å‹ç®€ä»‹é¡µä¼šæœ‰â€œ**æ”¯æŒè®­ç»ƒ**â€çš„æ ‡æ³¨ã€‚\n",
        "\n",
        "| é¢†åŸŸ | ä»»åŠ¡ç±»å‹      | æ¨¡å‹         |\n",
        "| --- |-----------|------------|\n",
        "| è‡ªç„¶è¯­è¨€å¤„ç† | æ–‡æœ¬åˆ†ç±»      | structbert |\n",
        "|  | æ–‡æœ¬åˆ†ç±»ï¼ˆå¤šè¯­è¨€ï¼‰ | veco       |\n",
        "|  | åºåˆ—æ ‡æ³¨      | structbert |\n",
        "|  | æ–‡æœ¬ç”Ÿæˆ      | palm_v2    |\n",
        "|  | æ–‡æœ¬ç”Ÿæˆ      | plug       |\n",
        "|  | æ–‡æœ¬åˆ†ç±»      | bert       |\n",
        "|  | åºåˆ—æ ‡æ³¨      | bert       |\n",
        "|  | é€šç”¨ç¿»è¯‘      | csanmt     |\n",
        "| å›¾åƒ | å›¾åƒå®ä¾‹åˆ†å‰²    | swin-b     |\n",
        "|  | å›¾åƒå»å™ªå£°     | nafnet     |\n",
        "|  | å›¾åƒé¢œè‰²å¢å¼º    | csrnet     |\n",
        "|  | äººåƒå¢å¼º      | gpen       |\n",
        "\n",
        "# è®­ç»ƒç»„ä»¶çš„è¯¦ç»†ä»‹ç»\n",
        "\n",
        "## æ›´æ–°è¯»å…¥é…ç½®çš„å‚æ•°\n",
        "\n",
        "å¾ˆå¤šæ—¶å€™é…ç½®æ–‡ä»¶çš„å‚æ•°æ— æ³•æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œæˆ–æŸäº›é…ç½®å‚æ•°ä¸€å®šè¦åœ¨è¿è¡Œæ—¶å¡«å…¥ï¼Œæ¯”å¦‚æ•°æ®é›†ä¿¡æ¯ã€é¢„å¤„ç†å™¨ä¼ å…¥çš„æ•°æ®é›†çš„keyã€optimizerçš„lambdaæ–¹æ³•ã€lr_schedulerçš„è¿­ä»£æ¬¡æ•°ç­‰ï¼Œè¿™æ—¶å°±éœ€è¦åœ¨è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´å‚æ•°ã€‚\n",
        "ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†é»˜è®¤PyTorch trainerä¸­æ›´æ–°å‚æ•°çš„ä¸€èˆ¬æ–¹å¼ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import os\n",
        "from modelscope.utils.hub import read_config\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "train_dataset = MsDataset.load('clue',  subset_name='afqmc', split='train')\n",
        "eval_dataset = MsDataset.load('clue',  subset_name='afqmc', split='validation')\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "# è¯»å–modelä¸­çš„cfgæ–‡ä»¶\n",
        "cfg = read_config(model_id)\n",
        "# ç›´æ¥æ›´æ–°å…¶ä¸­çš„å‚æ•°\n",
        "cfg.train.max_epochs = 5\n",
        "cfg.preprocessor.train['label2id'] = {'0': 0, '1': 1}\n",
        "cfg.preprocessor.val['label2id'] = {'0': 0, '1': 1}\n",
        "cfg.train.work_dir = '/tmp'\n",
        "cfg_file = os.path.join('/tmp', 'config.json')\n",
        "cfg.dump(cfg_file)\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    cfg_file=cfg_file)\n",
        "trainer = build_trainer(default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \u003cdiv id='load_dataset'\u003eè½½å…¥æ•°æ®é›†\u003c/div\u003e\n",
        "\n",
        "ModelScopeçš„è®­ç»ƒè¿‡ç¨‹æ”¯æŒä»å¤–éƒ¨ä¼ å…¥æ•°æ®é›†å®ä¾‹ï¼Œä¹Ÿæ”¯æŒå°†æ•°æ®é›†é…ç½®å†™å…¥é…ç½®æ–‡ä»¶ä¸­ç”±trainerè‡ªåŠ¨è¯»å–ã€‚æœ‰å…³æ•°æ®é›†åœ¨æ•°æ®æ–‡ä»¶ä¸­çš„é…ç½®å¯ä»¥å‚è€ƒ[æ•°æ®é›†çš„é…ç½®æ–‡ä»¶](../å¼€å‘è€…ä½¿ç”¨æŒ‡å—/Configurationè¯¦è§£.ipynb)ã€‚\n",
        "\n",
        "ä¸€èˆ¬è€Œè¨€ï¼ŒModelScopeæ¨¡å‹çš„é…ç½®ä¸­ä¸ä¼šåŒ…å«å…·ä½“çš„æ•°æ®é›†ä¿¡æ¯ï¼Œå¦‚æœæ‚¨éœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®æ•°æ®é›†ä¿¡æ¯ï¼ˆè€Œä¸æ˜¯ç›´æ¥å¤–éƒ¨åŠ è½½å¥½ä¼ å…¥trainerï¼‰ï¼Œä¸‹é¢å±•ç¤ºäº†ä¸€ç§åŸºæœ¬çš„åšæ³•ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import os\n",
        "from modelscope.utils.hub import read_config\n",
        "from modelscope.trainers import build_trainer\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "# è¯»å–modelä¸­çš„cfgæ–‡ä»¶\n",
        "cfg = read_config(model_id)\n",
        "# ç›´æ¥æ›´æ–°å…¶ä¸­çš„å‚æ•°\n",
        "cfg.train.max_epochs = 5\n",
        "cfg.train.work_dir = '/tmp'\n",
        "# ä¸ºpreprocessorè®¾ç½®å¥å­1ï¼Œå¥å­2ï¼Œæ ‡ç­¾çš„key\n",
        "cfg.preprocessor.train['label2id'] = {'0': 0, '1': 1}\n",
        "cfg.preprocessor.val['label2id'] = {'0': 0, '1': 1}\n",
        "cfg.preprocessor.label='label'\n",
        "# è®¾ç½®è®­ç»ƒæ•°æ®é›†å’ŒéªŒè¯æ•°æ®é›†\n",
        "cfg.dataset = {\n",
        "    'train': {\n",
        "        'name': 'clue',\n",
        "        'subset_name': 'afqmc',\n",
        "        'split': 'train',\n",
        "    },\n",
        "    'val': {\n",
        "        'name': 'clue',\n",
        "        'subset_name': 'afqmc',\n",
        "        'split': 'validation',\n",
        "    },\n",
        "}\n",
        "# å¦å­˜ä¸ºæ–°çš„é…ç½®\n",
        "cfg_file = os.path.join('/tmp', 'config.json')\n",
        "cfg.dump(cfg_file)\n",
        "# ä¼ å…¥cfg_fileï¼Œä¼ å…¥åcfg_fileä¼šæ›¿ä»£model_dirä¸­çš„cfg_file\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    cfg_file=cfg_file)\n",
        "trainer = build_trainer(default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "trainerå¯ä»¥æ”¯æŒè¾“å…¥æ•°æ®é›†æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåœ¨è¿™æ—¶ï¼Œtrainerå†…éƒ¨ä¼šä½¿ç”¨[TaskDataset](#TaskDataset)å¯¹æ•°æ®é›†è¿›è¡Œshuffleæ¥è®­ç»ƒã€‚\n",
        "\n",
        "## é¢„å¤„ç†å™¨\n",
        "\n",
        "æ•°æ®é¢„å¤„ç†è¿‡ç¨‹å¯ä»¥ç”±æ‚¨åœ¨è®­ç»ƒä¹‹å‰æ‰‹åŠ¨è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥ç”±traineråœ¨æ‰§è¡Œæ•°æ®è®­ç»ƒ/è¯„ä¼°æ—¶è‡ªåŠ¨è°ƒç”¨ã€‚trainerè‡ªåŠ¨è°ƒç”¨é¢„å¤„ç†è¿‡ç¨‹çš„ä¼ªä»£ç ç±»ä¼¼äºï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "for i, mini_batch in enumerate(data_loader):\n",
        "    mini_batch = preprocesser(mini_batch)\n",
        "    loss = model.forward(mini_batch)\n",
        "    ...\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "ä»¥ä¸‹åœºæ™¯å»ºè®®æ‚¨è®­ç»ƒä¹‹å‰æ‰‹åŠ¨è°ƒç”¨é¢„å¤„ç†è¿‡ç¨‹ï¼š\n",
        "- æ•°æ®é‡è¿‡å¤§ã€æ•°æ®å¤„ç†æ¯”è¾ƒå¤æ‚å¯¼è‡´è®­ç»ƒæ—¶é¢„å¤„ç†è¿‡ç¨‹è¾ƒå¤§å½±å“äº†è¿è¡Œæ—¶é•¿\n",
        "- æ•°æ®é¢„å¤„ç†çš„ä¸´æ—¶ç»“æœéœ€è¦è¢«ç¼“å­˜ä¸‹æ¥ä»¥å¤‡ä¸‹æ¬¡ä½¿ç”¨\n",
        "- é¢„å¤„ç†å™¨åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸å¥½æ„é€ å‡ºæ¥\n",
        "\n",
        "é™¤æ­¤ä¹‹å¤–çš„åœºæ™¯æ‚¨å¯ä»¥ä½¿ç”¨è‡ªåŠ¨é¢„å¤„ç†ï¼Œä»¥ç®€åŒ–è‡ªå®šä¹‰ä»£ç çš„å¤æ‚åº¦ã€‚\n",
        "\n",
        "è®­ç»ƒä¹‹å‰æ‰‹åŠ¨è°ƒç”¨é¢„å¤„ç†è¿‡ç¨‹çš„ä»£ç å¯ä»¥å‚è€ƒï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import os\n",
        "from modelscope.utils.hub import read_config, snapshot_download\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "from modelscope.preprocessors import Preprocessor\n",
        "from transformers import default_data_collator\n",
        "# MsDatasetå¯¹äºmapçš„æ”¯æŒæ¥è‡ªäºhuggingfaceçš„datasets\n",
        "train_dataset = MsDataset.load('clue', subset_name='afqmc', split='train').to_hf_dataset()\n",
        "eval_dataset = MsDataset.load('clue', subset_name='afqmc', split='validation').to_hf_dataset()\n",
        "model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'\n",
        "model_dir = snapshot_download(model_id)\n",
        "# è¯»å–modelä¸­çš„cfgæ–‡ä»¶\n",
        "cfg = read_config(model_dir)\n",
        "cfg.train.work_dir = '/tmp'\n",
        "# configçš„é¢„å¤„ç†å­—æ®µç½®ä¸ºç©º\n",
        "cfg.preprocessor = None\n",
        "cfg_file = os.path.join('/tmp', 'config.json')\n",
        "cfg.dump(cfg_file)\n",
        "\n",
        "train_preprocessor = Preprocessor.from_pretrained(model_dir,\n",
        "                                                  preprocessor_mode='train',\n",
        "                                                  first_sequence='sentence1',\n",
        "                                                  second_sequence='sentence2',\n",
        "                                                  label='label',\n",
        "                                                  label2id={'0':0, '1':1},\n",
        "                                                  sequence_length=256)\n",
        "eval_preprocessor = Preprocessor.from_pretrained(model_dir,\n",
        "                                                 preprocessor_mode='eval',\n",
        "                                                 first_sequence='sentence1',\n",
        "                                                 second_sequence='sentence2',\n",
        "                                                 label='label',\n",
        "                                                 label2id={'0':0, '1':1},\n",
        "                                                 sequence_length=256)\n",
        "train_dataset = train_dataset.map(train_preprocessor)\n",
        "eval_dataset = eval_dataset.map(eval_preprocessor)\n",
        "# ä¼ å…¥cfg_fileï¼Œä¼ å…¥åcfg_fileä¼šæ›¿ä»£model_dirä¸­çš„cfg_file\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    # trainerå†…éƒ¨ä½¿ç”¨äº†torch.utils.data.dataloader.default_collate, è¿™ä¸ªcollatorå’Œhfçš„listè¿”å›æ ¼å¼ä¸å…¼å®¹ï¼Œéœ€è¦ä½¿ç”¨hfè‡ªå·±çš„data_collator\n",
        "    data_collator=default_data_collator,\n",
        "    cfg_file=cfg_file)\n",
        "trainer = build_trainer(default_args=kwargs)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "æ€»ç»“æ¥è¯´ï¼Œé¢„å¤„ç†åœ¨è®­ç»ƒä¹‹å‰å®šä¹‰è¦æ³¨æ„å‡ ä¸ªåœ°æ–¹ï¼š\n",
        "1. å¦‚æœä½¿ç”¨MsDatasetè¿›è¡Œmapå¤„ç†éœ€è¦è°ƒç”¨to_hf_dataset\n",
        "2. cfgä¸­çš„preprocessoréœ€è¦ç½®ä¸ºç©ºå€¼\n",
        "3. data_collatoréœ€è¦è°ƒæ•´\n",
        "\n",
        "å¦‚æœåŠ è½½æ•°æ®é›†çš„è¿‡ç¨‹æ˜¯åœ¨é…ç½®æ–‡ä»¶ä¸­ç”±trainerè‡ªåŠ¨å®Œæˆï¼Œé‚£ä¹ˆtrainerä¹Ÿä¼šæ£€æŸ¥é¢„å¤„ç†å™¨åœ¨æ–‡ä»¶ä¸­çš„é…ç½®ï¼Œå¹¶è‡ªåŠ¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ï¼Œè¿™æ—¶è¯·æ³¨æ„æ›´æ–°preprocessoréƒ¨åˆ†çš„å€¼ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "# preprocessor_modeä¸éœ€è¦ä¼ å…¥ï¼Œå…¶ä»–çš„å€¼æ ¹æ®ç‰¹å®šæ¨¡æ€é€‰æ‹©å¡«å…¥\n",
        "cfg.preprocessor.first_sequence='sentence1'\n",
        "cfg.preprocessor.second_sequence='sentence2'\n",
        "cfg.preprocessor.label='label'\n",
        "cfg.preprocessor.label2id={'0':0, '1':1}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### å¦‚ä½•å®ç°ä¸€ä¸ªé¢„å¤„ç†å™¨\n",
        "\n",
        "å¦‚æœæ‚¨éœ€è¦å®ç°ä¸€ä¸ªè‡ªå·±çš„é¢„å¤„ç†å™¨ç”¨äºè®­ç»ƒæˆ–æ¨ç†ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](./æ•°æ®çš„é¢„å¤„ç†.ipynb)ã€‚\n",
        "\n",
        "## TaskDataset\n",
        "\n",
        "TaskDatasetæ˜¯ModelScopeç‰¹æœ‰çš„è®¾è®¡ã€‚å’Œæ¨¡å‹ã€é¢„å¤„ç†å™¨ä¸€æ ·ï¼Œè¿™ä¸ªç»„ä»¶ä¹Ÿæ˜¯é€šè¿‡æ³¨å†Œè¿›è¡Œè‡ªåŠ¨è°ƒç”¨çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æ˜¯åŠ è½½æ•°æ®é›†ã€é¢„å¤„ç†ã€è®­ç»ƒè¯„ä¼°ä¸‰éƒ¨åˆ†ï¼Œ\n",
        "ä½†åœ¨ä¸€äº›ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œæ•°æ®é›†éœ€è¦åœ¨ç‰¹å®šä»»åŠ¡æˆ–ç‰¹å®šæ¨¡å‹ä¸‹åˆç‰¹æ®Šçš„å¤„ç†ï¼Œä¸¾ä¾‹æ¥è¯´ï¼š\n",
        "1. æŸä¸ªæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦åŒæ—¶åŠ è½½å¤šä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œä»¥æŸç§æ–¹å¼è¿›è¡Œshuffleå†è¾“å…¥dataloader\n",
        "2. æŸä¸ªä»»åŠ¡éœ€è¦åŠ è½½ä¸€ä¸ªæ­£ä¾‹å’Œè‹¥å¹²ä¸ªè´Ÿä¾‹ï¼Œå¹¶å°†æ­£è´Ÿä¾‹åœ¨è¾“å…¥æ—¶è¿›å…¥ä¸€ä¸ªbatch\n",
        "\n",
        "TaskDatasetå°±æ˜¯ä¸ºäº†è¿™æ ·çš„åœºæ™¯è€Œè®¾è®¡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ»¡è¶³æŸäº›ä»»åŠ¡å’Œæ¨¡å‹çš„ç‰¹å®šæƒ…å¢ƒä¸‹éœ€è¦ï¼ˆå¤šä¸ªï¼‰æ•°æ®é›†æ•´ä½“è§†è§’ä¸‹çš„å¤„ç†ã€‚\n",
        "TaskDatasetçš„åŸºç±»å¦‚ä¸‹ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "class TaskDataset(ABC):\n",
        "    \"\"\"The task dataset base class for all the task specific dataset processors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 datasets: Union[Any, List[Any]],\n",
        "                 mode,\n",
        "                 preprocessor=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.preprocessor = preprocessor\n",
        "        self._inner_dataset = self.prepare_dataset(datasets)\n",
        "\n",
        "    @abstractmethod\n",
        "    def prepare_dataset(self, datasets: Union[Any, List[Any]]) -\u003e Any:\n",
        "        \"\"\"Prepare a dataset.\n",
        "\n",
        "        User can process the input datasets in a whole dataset perspective.\n",
        "        This method also helps to merge several datasets to one.\n",
        "\n",
        "        Args:\n",
        "            datasets: The original dataset(s)\n",
        "\n",
        "        Returns: A single dataset, which may be created after merging.\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def prepare_sample(self, data):\n",
        "        \"\"\"Preprocess the data fetched from the inner_dataset.\n",
        "\n",
        "        If the preprocessor is None, the original data will be returned, else the preprocessor will be called.\n",
        "        User can override this method to implement custom logics.\n",
        "\n",
        "        Args:\n",
        "            data: The data fetched from the dataset.\n",
        "\n",
        "        Returns: The processed data.\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "å¯ä»¥çœ‹åˆ°ï¼ŒTaskDataseté»˜è®¤å…³æ³¨modeï¼ˆè®­ç»ƒè¿˜æ˜¯è¯„ä¼°ï¼‰ï¼Œæ˜¯å¦æœ‰é¢„å¤„ç†è¿‡ç¨‹ï¼Œä»¥åŠå®é™…è¦æ“ä½œçš„æ•°æ®é›†ã€‚\n",
        "trainerå¯ä»¥æ¥å—ä¸€ä¸ªTaskDatasetç±»å‹çš„è¾“å…¥ï¼ˆå–ä»£åŸæœ¬çš„æ•°æ®é›†è¾“å…¥ï¼‰ï¼Œä½†æˆ‘ä»¬ä¸€èˆ¬å»ºè®®ç”±traineråœ¨å†…éƒ¨è‡ªåŠ¨è°ƒèµ·TaskDatasetï¼Œç”¨æˆ·åªéœ€è¦ä¼ å…¥è‡ªå·±çš„æ•°æ®é›†å¯¹è±¡å³å¯ã€‚\n",
        "åœ¨è¿™æ—¶ï¼Œmodeå’Œpreprocessorï¼ˆå¦‚trainerä¸­å­˜åœ¨ï¼‰ä¼šç”±trainerè‡ªåŠ¨ä¼ å…¥ï¼Œ_inner_datasetæ˜¯æ‚¨åœ¨æ„é€ ä¸­æˆ–é…ç½®ä¸­å®šä¹‰çš„è®­ç»ƒ/è¯„ä¼°æ•°æ®é›†ï¼Œä¹Ÿä¼šè¢«trainerä¼ å…¥initä¸­ã€‚\n",
        "\n",
        "TaskDatasetçš„æ³¨å†Œæ˜¯ä»»åŠ¡+æ¨¡å‹å¼çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œåªè¦åœ¨æ¨¡å‹æ–‡ä»¶ä¸­æŒ‡å®šäº†æ¨¡å‹åç§°å’Œä»»åŠ¡æ¨¡å‹ï¼Œåœ¨è®­ç»ƒæ—¶å¯¹åº”çš„TaskDatasetå°±ä¼šè¢«ä½¿ç”¨ã€‚\n",
        "\n",
        "### å†™ä¸€ä¸ªæ–°çš„TaskDataset\n",
        "å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯Pytorchï¼Œæ‚¨å¯ä»¥ç›´æ¥ç»§æ‰¿TorchTaskDatasetï¼Œå®ƒç»§æ‰¿äº†PyTorchçš„Datasetï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.msdatasets.task_datasets import TorchTaskDataset\n",
        "from typing import Any, List, Union\n",
        "class CustomDataset(TorchTaskDataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 datasets: Union[Any, List[Any]],\n",
        "                 mode,\n",
        "                 preprocessor=None,\n",
        "                 **kwargs):\n",
        "        super().__init__(datasets, mode, preprocessor, **kwargs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # TODO Write how to fetch an data item here\n",
        "        pass\n",
        "\n",
        "    def prepare_dataset(self, datasets: Union[Any, List[Any]]) -\u003e Any:\n",
        "        # TODO This will be called when the trainer is initing, so you can \n",
        "        # write how to mix or prepare the input datasets.\n",
        "        pass\n",
        "    def prepare_sample(self, data):\n",
        "        # how to prepare the sample, by default it will call the preprocessor(if exists in trainer) to do this.\n",
        "        pass\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "åœ¨æ‚¨çš„mainæ–¹æ³•æˆ–å…¶ä»–ä¼šè¢«æ‰§è¡Œçš„åœ°æ–¹è°ƒç”¨ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.msdatasets.task_datasets import TASK_DATASETS\n",
        "TASK_DATASETS.register_module(module_name='my-custom-model', group_key='my-custom-task', module_cls=CustomDataset)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "è¿™æ ·ï¼Œåœ¨trainerä½¿ç”¨æ‚¨çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶å°±ä¼šè‡ªåŠ¨åŠ è½½è¿™ä¸ªTaskDatasetäº†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTaskDataset initç»“æŸåä¼šè¢«ä¼ å…¥trainerå¯¹è±¡ï¼Œæ‚¨å¯ä»¥\n",
        "åœ¨æ‚¨çš„TaskDatasetå†…éƒ¨ä½¿ç”¨å®ƒï¼Œæ¯”å¦‚è·å¾—é…ç½®ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "print(self.trainer.cfg)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## æ¨¡å‹çš„éªŒè¯\n",
        "å¾ˆå¤šæ¨¡å‹çš„é…ç½®ä¸­ï¼Œè®­ç»ƒè¿‡ç¨‹æ²¡æœ‰é…ç½®äº¤å‰éªŒè¯ã€‚å¦‚æœæ‚¨æƒ³åœ¨è®­ç»ƒæ—¶åŒæ­¥è¿›è¡Œäº¤å‰éªŒè¯å¯ä»¥åœ¨configuration.jsonä¸­æ·»åŠ ä¸€ä¸ªEvaluationHookï¼Œå…·ä½“é…ç½®å¦‚ä¸‹ï¼š\n",
        "```json\n",
        "{\n",
        "   ...\n",
        "  \"train\": {\n",
        "     ...\n",
        "      \"hooks\": [\n",
        "          ...\n",
        "        {\n",
        "          \"type\": \"EvaluationHook\",\n",
        "          # æ˜¯å¦æ¯ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯ï¼Œfalseçš„æ—¶å€™ä»£è¡¨æŒ‰iteréªŒè¯\n",
        "          \"by_epoch\": false,\n",
        "          # å¤šå°‘ä¸ªepoch/iterè¿›è¡Œä¸€æ¬¡éªŒè¯\n",
        "          \"interval\": 100\n",
        "      }]\n",
        "  },\n",
        "\n",
        "}\n",
        "```\n",
        "\n",
        "ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´é…ç½®æ–‡ä»¶ï¼Œä¹Ÿå¯è‡ªè¡Œæ³¨å†Œç›¸åº”hookï¼Œå¹¶é€šè¿‡typeå­—æ®µæ³¨å†Œåœ¨é…ç½®æ–‡ä»¶ä¸­è¿›è¡Œè°ƒç”¨ã€‚\n",
        "å…³äºhookçš„è¯¦ç»†è¯´æ˜è¯·å‚è€ƒæ–‡æ¡£ï¼š[å›è°ƒå‡½æ•°æœºåˆ¶è¯¦è§£](../å¼€å‘è€…ä½¿ç”¨æŒ‡å—/å›è°ƒå‡½æ•°æœºåˆ¶è¯¦è§£.ipynb)\n",
        "\n",
        "\n",
        "## æ¨¡å‹çš„ä¿å­˜ä¸åç»­ä½¿ç”¨\n",
        "\n",
        "æ¨¡å‹è®­ç»ƒç»“æŸåï¼ˆæˆ–æŠ›å¼‚å¸¸åï¼‰ï¼Œéƒ½ä¼šæœ‰åç»­çš„å…¶ä»–å¤„ç†ï¼Œæ¯”å¦‚è®­ç»ƒè¿‡ç¨‹æŠ›å¼‚å¸¸åæ‚¨å¯èƒ½éœ€è¦åŠ è½½æœ€åä¸€ä¸ªä¿å­˜ä¸‹æ¥çš„æ¨¡å‹è¿›è¡Œç»§ç»­è®­ç»ƒï¼Œæ¯”å¦‚è®­ç»ƒå®Œæˆåæ‚¨å¯èƒ½\n",
        "éœ€è¦å°†å…¶ç”¨äºæ¨ç†ä¸­æˆ–å¯¼å‡ºä¸ºä¸­é—´æ ¼å¼ä¾¿äºåœ¨C++ç¯å¢ƒä¸­ä½¿ç”¨æˆ–è¿›è¡ŒåŠ é€Ÿã€‚ModelScopeçš„traineræ”¯æŒæ‚¨è¿›è¡Œä¸Šè¿°çš„ä»»ä½•ä¸€ç§å®æ“ã€‚\n",
        "\n",
        "### æ¨¡å‹ä¿å­˜çš„é…ç½®\n",
        "\n",
        "ModelScopeæ”¯æŒä¸¤ç§æ¨¡å‹ä¿å­˜çš„Hookï¼Œåˆ†åˆ«æ˜¯CheckpointHookå’ŒBestCkptSaverHookã€‚ \n",
        "\n",
        "#### CheckpointHook\n",
        "\n",
        "è¿™æ˜¯ModelScope`é»˜è®¤`ä½¿ç”¨çš„å­˜å‚¨æ¨¡å‹hookï¼Œå¦‚æœæ‚¨åœ¨é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰é…ç½®ï¼Œå®ƒä¼šè¢«é»˜è®¤æ·»åŠ åˆ°hooksä¸­ï¼Œå®ƒçš„ä½œç”¨æ˜¯æ¯ä¸ªepochå­˜å‚¨ä¸€æ¬¡æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³è¦è¦†ç›–è¿™ä¸€æ•ˆæœï¼Œ\n",
        "æ‚¨å¯ä»¥è¿™æ ·é…ç½®ï¼š\n",
        "```json\n",
        "{\n",
        "   ...\n",
        "  \"train\": {\n",
        "     ...\n",
        "      \"hooks\": [\n",
        "          ...\n",
        "        {\n",
        "            # Save every 100 iters(not epochs)\n",
        "            \"type\": \"CheckpointHook\",\n",
        "            \"by_epoch\": false,\n",
        "            \"interval\": 100\n",
        "        }]\n",
        "  },\n",
        "\n",
        "}\n",
        "```\n",
        "\n",
        "#### BestCkptSaverHook\n",
        "\n",
        "ä¸€èˆ¬æ¥è¯´ç”¨æˆ·å¯èƒ½ä¼šå¸Œæœ›åªå­˜å‚¨æœ€ä½³çš„æ¨¡å‹æ–‡ä»¶ï¼ŒModelScopeä¹Ÿæä¾›äº†å¯¹åº”çš„æœºåˆ¶æ¥è¾¾åˆ°è¿™ä¸€ç›®çš„ã€‚\n",
        "- é¦–å…ˆéœ€è¦ä¿è¯eval_datasetä¼šè¢«ä¼ å…¥trainerï¼ˆæˆ–é€šè¿‡é…ç½®æ–‡ä»¶çš„æ–¹å¼ï¼‰\n",
        "- é…ç½®æ–‡ä»¶å¢åŠ æˆ–æ”¹ä¸ºBestCkptSaverHook\n",
        "\n",
        "```json\n",
        "{\n",
        "   ...\n",
        "  \"train\": {\n",
        "     ...\n",
        "      \"hooks\": [\n",
        "          ...\n",
        "        {\n",
        "            \"type\": \"BestCkptSaverHook\",\n",
        "            \"metric_key\": \"accuracy\",\n",
        "            \"by_epoch\": false,\n",
        "            \"rule\": \"max\",\n",
        "            \"interval\": 100\n",
        "        }]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "\"metric_key\"ç”¨æ¥è¡¨ç¤ºMetricè¿”å›å€¼ä¸­ç”¨æ¥å®é™…è¿›è¡Œæ¯”è¾ƒçš„keyï¼Œä»¥\"rule\"çš„æ–¹å¼æ¯”è¾ƒå¹¶å­˜å‚¨æ¨¡å‹æ–‡ä»¶ã€‚æœ‰å…³Metricçš„å…·ä½“ä½¿ç”¨å¯ä»¥å‚è€ƒ[è¿™é‡Œ](./æ¨¡å‹çš„è¯„ä¼°.ipynb)ã€‚\n",
        "\n",
        "#### ä½¿ç”¨æ–‡ä»¶è¿›è¡Œè®­ç»ƒæˆ–æ¨ç†\n",
        "\n",
        "ä»¥ä¸Šä»»æ„ä¸€ç§CkptHookéƒ½ä¼šå­˜å‚¨ä¸¤ç§ç±»å‹çš„æ–‡ä»¶ã€‚\n",
        "\n",
        "- ç”¨æ¥æ¢å¤è®­ç»ƒçš„æ–‡ä»¶\n",
        "åœ¨å¯¹åº”hookçš„save_dirï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå³ä¸ºtrainer.work_dirï¼‰ä¸­ï¼Œå­˜åœ¨åç¼€åä¸ºpthçš„æ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶æ˜¯ç”¨æ¥æ¢å¤è®­ç»ƒçš„ã€‚æ–‡ä»¶é‡Œé¢å­˜å‚¨äº†æ¨¡å‹å‚æ•°ã€optimizerå‚æ•°ã€\n",
        "lr_schedulerå‚æ•°ã€éšæœºçŠ¶æ€ã€trainerçŠ¶æ€ç­‰ï¼ŒåŠ è½½åå°±å¯ä»¥ç»§ç»­è®­ç»ƒï¼ˆåŒ…æ‹¬dataloaderçš„åç»­è¯»å–æ•°æ®ï¼‰ã€‚åŠ è½½æ–¹å¼å¯ä»¥å‚è€ƒï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "import os\n",
        "trainer.train(os.path.join(trainer.work_dir, 'a-pth-file.pth'))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "- ç”¨æ¥æ¨ç†çš„æ–‡ä»¶\n",
        "åœ¨å¯¹åº”çš„save_dirä¸­ä¼šå­˜åœ¨ä¸€ä¸ªoutputæ–‡ä»¶å¤¹ï¼Œæ‰“å¼€åä¼šå‘ç°é‡Œé¢çš„æ–‡ä»¶æ ¼å¼å’Œæ¨ç†ä½¿ç”¨çš„æ–‡ä»¶ç›¸åŒï¼Œæ‚¨å¯ä»¥åœ¨pipelineä¸­ç›´æ¥åŠ è½½è¿™ä¸ªæ–‡ä»¶å¤¹è¿›è¡Œæ¨ç†ã€‚\n",
        "æœ‰å…³pipelineçš„ä½¿ç”¨å‚è€ƒ[è¿™é‡Œ](./æ¨¡å‹çš„æ¨ç†Pipeline.ipynb)ã€‚\n",
        "\n",
        "#### ä½¿ç”¨å…¶ä»–æ¡†æ¶çš„datasetè¿›è¡Œè®­ç»ƒ\n",
        "\n",
        "ModelScopeæ”¯æŒä½¿ç”¨PyTorchæ¡†æ¶çš„Datasetæˆ–å…¶ä»–è‡ªå®šä¹‰çš„Datasetè¿›è¡Œè®­ç»ƒï¼Œåªéœ€è¦è¯¥Datasetå…·æœ‰__getitem__æ–¹æ³•ï¼ˆæˆ–è€…è¯´ï¼Œæ”¯æŒå¯ä»¥ä¼ å…¥torchçš„DataLoaderä¸­å³å¯ï¼‰ã€‚\n",
        "æ‚¨åªéœ€è¦å°†è¯¥datasetä¼ å…¥traineræ„é€ æ–¹æ³•çš„train_datasetå’Œeval_datasetå‚æ•°ä¸­ã€‚\n",
        "\n",
        "#### ä½¿ç”¨torchçš„Moduleè¿›è¡Œè®­ç»ƒ\n",
        "\n",
        "ModelScopeçš„traineræ”¯æŒè®­ç»ƒtorch.nn.Moduleï¼Œå‰ææ˜¯æ‚¨å¿…é¡»ä¼ å…¥å¯¹åº”çš„configuration.jsonã€‚\n",
        "\n",
        "## æ•°æ®å¹¶è¡Œ\n",
        "\n",
        "More information neededã€‚\n",
        "\n",
        "## ä½¿ç”¨deepspeedç­‰åˆ†å¸ƒå¼æ¡†æ¶è®­ç»ƒå¤§æ¨¡å‹\n",
        "\n",
        "More information neededã€‚\n",
        "\n",
        "## å„æ¨¡æ€çš„trainer\n",
        "\n",
        "### NLP\n",
        "\n",
        "åœ¨NLPåœºæ™¯ä¸‹ï¼ŒModelScopeæä¾›äº†é¢†åŸŸå‹å¥½çš„ç‰¹å®štrainerï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªå…·ä½“çš„sampleï¼š\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.metainfo import Preprocessors\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import build_trainer\n",
        "from modelscope.utils.constant import Tasks\n",
        "\n",
        "\n",
        "# é€šè¿‡è¿™ä¸ªæ–¹æ³•ä¿®æ”¹config\n",
        "def cfg_modify_fn(cfg):\n",
        "    cfg.task = Tasks.sentence_similarity\n",
        "    cfg['preprocessor'] = {'type': Preprocessors.sen_sim_tokenizer}\n",
        "    cfg.train.optimizer.lr = 2e-5\n",
        "    cfg['dataset'] = {\n",
        "        'train': {\n",
        "            # å®é™…labelå­—æ®µå†…å®¹æšä¸¾ï¼Œåœ¨è®­ç»ƒbackboneæ—¶éœ€è¦ä¼ å…¥\n",
        "            'labels': ['0', '1'],\n",
        "            # ç¬¬ä¸€ä¸ªå­—æ®µçš„key\n",
        "            'first_sequence': 'sentence1',\n",
        "            # ç¬¬äºŒä¸ªå­—æ®µçš„key\n",
        "            'second_sequence': 'sentence2',\n",
        "            # labelçš„key\n",
        "            'label': 'label',\n",
        "        }\n",
        "    }\n",
        "    cfg.train.max_epochs = 5\n",
        "    cfg.train.lr_scheduler = {\n",
        "        'type': 'LinearLR',\n",
        "        'start_factor': 1.0,\n",
        "        'end_factor': 0.0,\n",
        "        'total_iters':\n",
        "            int(len(dataset['train']) / 32) * cfg.train.max_epochs,\n",
        "        'options': {\n",
        "            'by_epoch': False\n",
        "        }\n",
        "    }\n",
        "    cfg.train.hooks = [{\n",
        "        'type': 'CheckpointHook',\n",
        "        'interval': 1\n",
        "    }, {\n",
        "        'type': 'TextLoggerHook',\n",
        "        'interval': 1\n",
        "    }, {\n",
        "        'type': 'IterTimerHook'\n",
        "    }, {\n",
        "        'type': 'EvaluationHook',\n",
        "        'by_epoch': False,\n",
        "        'interval': 100\n",
        "    }]\n",
        "    return cfg\n",
        "\n",
        "\n",
        "work_dir = 'tmp'\n",
        "model_id = 'damo/nlp_structbert_backbone_base_std'\n",
        "dataset = MsDataset.load('clue', subset_name='afqmc')\n",
        "kwargs = dict(\n",
        "    model=model_id,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    work_dir=work_dir,\n",
        "    seed=42,\n",
        "    cfg_modify_fn=cfg_modify_fn)\n",
        "\n",
        "# ä½¿ç”¨nlp-base-trainer\n",
        "trainer = build_trainer(name='nlp-base-trainer', default_args=kwargs)\n",
        "# è¯„ä¼°åŸå§‹æ¨¡å‹\n",
        "origin_result = trainer.evaluate()\n",
        "# finetune\n",
        "trainer.train()\n",
        "# è¯„ä¼°finetuneåçš„æ¨¡å‹\n",
        "finetuned_result = trainer.evaluate()\n",
        "print(f\"åŸå§‹æ¨¡å‹çš„è¯„ä¼°ç»“æœä¸º{origin_result}, finetuneåæ¨¡å‹è¾¾åˆ°{finetuned_result}\")\n",
        "\n",
        "# ç›´æ¥è¿è¡Œæœ¬ç¤ºä¾‹å¾—åˆ°è¾“å‡ºï¼š\n",
        "# åŸå§‹æ¨¡å‹çš„è¯„ä¼°ç»“æœä¸º{'accuracy': 0.31000927090644836}, finetuneåæ¨¡å‹è¾¾åˆ°{'accuracy': 0.7509267926216125}\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "nlp-base-trainerç»§æ‰¿äº†é»˜è®¤çš„trainerï¼Œå¯¹äºnlpåŸŸå†…å¤šæä¾›äº†ä»¥ä¸‹åŠŸèƒ½ï¼š\n",
        "\n",
        "- æä¾›äº†cfg_modify_fnå‚æ•°ï¼Œç”¨æˆ·å¯ä»¥åœ¨è°ƒç”¨trainerä¹‹å‰åœ¨ä»£ç ä¸­ä¿®æ”¹cfgï¼Œå¹¶ä¸”åŸºäºcfg_modify_fnæä¾›äº†`NlpTrainerArguments`ç±»å°†å‚æ•°æ‰å¹³åŒ–ä¼ å…¥\n",
        "- åœ¨initæ—¶ä¼šå°è¯•è¯»å–configuration.json/config.json/label_mapping.jsonï¼Œå°†labelæ•°é‡ä¿¡æ¯ï¼ˆnum_labelsï¼‰ä¼ é€’ç»™æ¨¡å‹ï¼Œå°†label2idä¼ é€’ç»™preprocessor\n",
        "- æ”¯æŒåœ¨cfgçš„datasetå­—æ®µä¸­é…ç½®first_sequenceã€second_sequenceã€labelä¸‰ä¸ªå­—æ®µçš„ä¿¡æ¯ï¼Œä»¥å…æ±¡æŸ“preprocessoré…ç½®ï¼Œæ”¯æŒåœ¨datasetå­—æ®µä¸­é…ç½®labelsï¼Œè¯¥labelsä¿¡æ¯ä¼šè¢«è§£ææˆlabel2idå’Œnum_labels\n",
        "\n",
        "`NlpTrainerArguments`æ˜¯ä¸€ä¸ªå¸¦æœ‰__call__æ–¹æ³•çš„dataclassç±»ï¼Œæ˜¯å‚æ•°é…ç½®æ–¹æ³•ï¼ˆcfg_modify_fnï¼‰çš„é«˜çº§å°è£…ã€‚\n",
        "å®ƒåƒdataclassä¸€æ ·æ¥å—å‚æ•°ï¼Œå¹¶å¯ä»¥ä½œä¸ºcallbackä¼ é€’ç»™cfg_modify_fnæ¥ä¿®æ”¹cfgã€‚æˆ‘ä»¬æ¨èæ‚¨ä¼˜å…ˆä½¿ç”¨å®ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true
      },
      "source": [
        "from modelscope.metainfo import Preprocessors\n",
        "from modelscope.metainfo import Trainers\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.trainers import NlpTrainerArguments\n",
        "from modelscope.trainers import build_trainer\n",
        "from modelscope.utils.constant import Tasks\n",
        "\n",
        "dataset = MsDataset.load('clue', subset_name='tnews')\n",
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "cfg_modify_fn = NlpTrainerArguments(\n",
        "    # taskä¿¡æ¯\n",
        "    task=Tasks.text_classification,\n",
        "    # é¢„å¤„ç†å™¨ç±»å‹\n",
        "    preprocessor_type=Preprocessors.sen_cls_tokenizer,\n",
        "    # è®­ç»ƒæ•°æ®é›†çš„å¥å­1\n",
        "    train_first_sequence='sentence',\n",
        "    # è®­ç»ƒæ•°æ®é›†çš„label\n",
        "    train_label='label',\n",
        "    # labelsåˆ—è¡¨\n",
        "    labels=[\n",
        "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                '11', '12', '13', '14'\n",
        "            ],\n",
        "    # æœ€å¤§è¿­ä»£è½®æ•°\n",
        "    max_epochs=5,\n",
        "    # ä¼˜åŒ–å™¨å‚æ•°ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªdictã€‚å¯ä»¥åªä¼ å…¥å…³å¿ƒçš„å€¼ï¼Œå¸¸è§çš„å¦‚lr\n",
        "    optimizer_args={\n",
        "        'lr': 3e-5,\n",
        "    },\n",
        "    # lr_schedulerå‚æ•°ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªdictã€‚å¯ä»¥åªä¼ å…¥å…³å¿ƒçš„å€¼ï¼Œå¦‚é»˜è®¤é…ç½®ä¸­çš„LinearLRçš„total_iterså‚æ•°\n",
        "    lr_scheduler_args={\n",
        "        'total_iters': int(len(train_dataset) / 32) * 5,\n",
        "    },\n",
        "    # å­˜å‚¨Ckptçš„Hook\n",
        "    checkpoint_saving_type='BestCkptSaverHook',\n",
        "    # BestCkptSaverHookåˆ¤æ–­æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹æ—¶ä½¿ç”¨çš„Metric key\n",
        "    metric_key='accuracy',\n",
        "    train_batch_size_per_gpu=32,\n",
        "    checkpoint_interval=1,\n",
        "    train_workers_per_gpu=0,\n",
        "    checkpoint_by_epoch=False,\n",
        "    evaluation_interval=1,\n",
        "    evaluation_by_epoch=False,\n",
        "    eval_workers_per_gpu=0,\n",
        "    metrics=['seq-cls-metric'],\n",
        ")\n",
        "\n",
        "kwargs = dict(\n",
        "    model='damo/nlp_structbert_backbone_base_std',\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    work_dir='/tmp',\n",
        "    seed=42,\n",
        "    cfg_modify_fn=cfg_modify_fn)\n",
        "\n",
        "trainer = build_trainer(name=Trainers.nlp_base_trainer, default_args=kwargs)\n",
        "\n",
        "# è¯„ä¼°åŸå§‹æ¨¡å‹\n",
        "origin_result = trainer.evaluate()\n",
        "# finetune\n",
        "trainer.train()\n",
        "# è¯„ä¼°finetuneåçš„æ¨¡å‹\n",
        "finetuned_result = trainer.evaluate()\n",
        "print(f\"åŸå§‹æ¨¡å‹çš„è¯„ä¼°ç»“æœä¸º{origin_result}, finetuneåæ¨¡å‹è¾¾åˆ°{finetuned_result}\")\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªç±»çš„æ‰€æœ‰å€¼é»˜è®¤æ˜¯Noneï¼Œè¿™æ—¶ä»£è¡¨ä½¿ç”¨configuration.jsonä¸­çš„é»˜è®¤è®¾ç½®ã€‚\n",
        "æœ‰å…³NlpTrainerArgumentsçš„å…·ä½“è®¾ç½®æ‚¨å¯ä»¥æŸ¥çœ‹`modelscope.trainers.nlp_trainer.py`ã€‚\n",
        "\n",
        "nlp-base-trainerçš„å…¶ä»–æ–¹æ³•å’Œæ„é€ æ–¹æ³•å‚æ•°å’Œé»˜è®¤trainerç›¸åŒï¼Œæ‚¨å¯ä»¥æŒ‰ç…§é»˜è®¤trainerçš„æ–¹å¼ä½¿ç”¨nlp-base-trainerã€‚\n",
        "å¦‚æœæ‚¨ä½¿ç”¨nlpçš„æ¨¡å‹å¹¶ä¸”nlp-base-traineræ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨èæ‚¨åœ¨åŸŸå†…ä¼˜å…ˆä½¿ç”¨å®ƒã€‚\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
